


% ============================================================================
\documentclass[11pt,letterpaper]{article}
%\usepackage[T1]{fontenc}
%\usepackage[latin1]{inputenc}
\usepackage{epic,eepic,amsmath,latexsym,amssymb,color,amsthm}
\usepackage{ifthen,graphics,epsfig,fullpage} 
\usepackage[english]{babel} 
\bibliographystyle{plain}
\usepackage{times}


% =========================================================================
\newcommand{\Xomit}[1]{}
\newcommand{\ignore}[1]{}
% =========================================================================


\begin{document}

%-----------------------for square--------------------------------------------
\newlength {\squarewidth}
\renewenvironment {square}
{
\setlength {\squarewidth} {\linewidth}
\addtolength {\squarewidth} {-12pt}
\renewcommand{\baselinestretch}{0.75} \footnotesize
\begin {center}
\begin {tabular} {|c|} \hline
\begin {minipage} {\squarewidth}
\medskip
}{
\end {minipage}
\\ \hline
\end{tabular}
\end{center}
}  
 
%--------------------------------------------------------------------
%--------------------------------------------------------------------
%-------- macros for algorithm ---------------------------------------
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newcommand{\toto}{xxx}
\newenvironment{proofT}{\noindent{\bf
Proof }} {\hspace*{\fill}$\Box_{Theorem~\ref{\toto}}$\par\vspace{3mm}}
\newenvironment{proofL}{\noindent{\bf
Proof }} {\hspace*{\fill}$\Box_{Lemma~\ref{\toto}}$\par\vspace{3mm}}
\newenvironment{proofC}{\noindent{\bf
Proof }} {\hspace*{\fill}$\Box_{Corollary~\ref{\toto}}$\par\vspace{3mm}}


\newcounter{linecounter}
\newcommand{\linenumbering}{\ifthenelse{\value{linecounter}<10}
{(0\arabic{linecounter})}{(\arabic{linecounter})}}
\renewcommand{\line}[1]{\refstepcounter{linecounter}\label{#1}\linenumbering}
\newcommand{\resetline}[1]{\setcounter{linecounter}{0}#1}
\renewcommand{\thelinecounter}{\ifnum \value{linecounter} > 
9\else 0\fi \arabic{linecounter}}

\newcommand{\tuple}[1]{\ensuremath{\left \langle #1 \right \rangle }}

%----------------------------------------------------------------------

Originally proposed as a hardware mechanism for easily developing non-blocking data structures,
Transactional Memory (TM) has since evolved to become a general purpose method for more easily writing
efficient concurrent code.
With TM, a programmer is able to declare atomic transactions, more specifically, blocks
of one or several reads and writes to shared memory bounded by calls to the
$\mathit{begin\_transaction}$ and $\mathit{end\_transaction}$ operations.
These reads and writes then appear to execute in exclusion with no interleaving
between other transactions, as if a single global lock had been used for each transaction.

Unlike using a global lock, several transactions can be executed at the same time,
with the system ensuring that they appear to have executed in exclusion.
In order to still ensure this, the TM system will keep track of reads and writes
performed by transactions, detecting conflicts, and aborting and restarting transactions
as necessary.
Defining precisely how transactions execute in relation to each other is a consistency criterion.
Probably the most commonly used criterion, linearisability, roughly says that each
operation appears to have occurred at some instant in time between its invocation and completion.
Opacity, the most common criterion used for TM, ensures this for both aborted and committed transactions.

For certain applications, certain STM systems exhibit performance that scales as the number
of processing core increases.
But this is the exception, not the norm, it the general case STM is considered to be slow
and unscalable.
While numerous algorithms and enhancements have been proposed in order to improve the performance
of STM, they are still usually advantageous in certain situations.
The goal of this work is goes a step further in a attempt to improve performance, instead of 
just tweaking algorithms and ensuring the same guarantees, it weakens the model of transactions
from being totally ordered atomic blocks, to a model that allows concurrent conflicting transactions
both execute and commit successfully with their updates being merged together.
This will require the programmer to understand more complex concepts and limit the transactions
to certain data types with the aspired trade off of increased scalability.

\subsection{Opacity}
How a transaction executes in relation to another transaction is defined by a consistency condition,
allowing a programmer to know how he can use transactions to correctly write his programs.
The most commonly used consistency condition for concurrent implementations of abstractions or data types
is \emph{linearisability}.
When an implementation of a data type ensures linearisability then each operation executed on it will
be totally ordered based on real time, where an operation appears to have completely executed at
some instant in time (called a linearisation point) between it invocation and completion.
Specifically, when considering TM, a bit of a stronger condition is used called \emph{opacity}.
The reason for this is that in TM there is the concept of an aborted transaction.
That is a transaction that has started executing, but due to conflicts due to concurrency
has had to stop and start over (otherwise risking violating the atomicity of transactions),
leaving no visible effect on the state of the shared data.
Even though linearisability will ensure the commuted transactions are correct, aborted transactions
might see invalid states of memory before they abort, risking problems such a divide by zero exceptions
or 
Opacity prevents this by requiring the reads performed by aborted transactions to be exists
as totally ordered atomic transactions along with the committed transactions.

For the programmer, this means that the reads and writes of a transaction
appear to all happen atomically at some instant, preventing him from having
to worry about different interleaving of reads and writes between concurrent transactions.
Given that from within a transaction, a programmer can call a read to any location in shared memory
it can become costly for the system to ensure opacity in the presence of concurrency.
There are dozens of different STM algorithms that try to minimize these cost with the hope
of improving performance, scalability, or progress.
On one extreme an algorithm may keep large amounts of meta data to try to make
sure that every transaction can commit if it was possible, but keeping and processing
this meta data becomes expensive.
On another extreme an algorithm might try to keep operations as light as possible, but
then might have to abort transactions unnecessarily just in case they might violate
opacity, limiting concurrency.
Other algorithms have been proposed doing everything in-between.


\subsection{Weakened consistency}
\paragraph{Need to finish this section!}
While strong consistency such as opacity makes concurrent programs easier to write,
it can limit performance and scalability.
This ease of use might be essential for certain developers, but others might be willing
to give up some of the ease of use in order to 

\paragraph{Point 1:} Need to compare to eventual consistency in distributed systems, showing
that they had advantages there, and can maybe apply to shared memory abstractions.

\paragraph{Point 2:} Need to reference other weakened consistency protocols that have been used
for TM and show how their are different and where they have advantages/weaknesses.

\paragraph{Point 3:} Need to decide is the type of transaction consistency provided in these
protocols is actually something interesting for scalability and applications.
This can be done partly by benchmarking and redesigning the algorithms as necessary.
Should look into parallel graph processing algorithms from PPoPP for example.


\section{Stop the world}

The most direct way to implement the STM system is to allow all processes
execute concurrently in isolation without making any modifications to shared memory.
A single copy of the data structure is store in shared memory that is not modified.
By avoiding modifications to this data structure, any group of reads to this data
performed during a transaction will be guaranteed to provide a consistent snapshot.
Updates done by transactions are stored locally.

Eventually these updates must be shared so that they become visible to the other processes.
This is done in a simple ``stop the world'' fashion.
A separate process will tell every process to complete their current transaction (if there is one) and then to pause
their execution.
Once all processes are stopped, the separate process will go through all the updates that are stored locally at
each process and, using the communitive properties of the CRDT? model, will merge them to the shared copy.
Updates done by each process are considered ``concurrent'' with the updates of all other processes and are merged
accordingly, for example by using the add wins model of a concurrent set.

Once all updates have been merged to the shared copy, the processes are allowed to continue their execution,
with transactions reading from the new version of the data.


\subsection{Operations and data structures}

The shared copy of the data is stored in a hash table
$\mathit{hash\_table}$ with each item in the hash-table
being stored as an object
containing a value $\mathit{val}$ and
being identified by a unique address $\mathit{addr}$.

Each process has several local variables.
The boolean status variables $\mathit{stopped}$,
$\mathit{stop}$, and $\mathit{live}$ are used to indicate
where or not the process is currently executing a transaction
or is waiting for the shared merging to complete.
Additionally each process contains a write set $\mathit{WS}$
implemented as a hash table
storing all updates that have taken place on this process
since the last merge operation took place.

\paragraph{begin\_transaction}
The $\mathit{begin\_transaction}$ operation is the entrance to
a transaction as indicated by the programmer.
Its main purpose is to announce that this thread will be executing
a transaction, so that merging to shared memory does not take place
while it is executing.
Additionally, if a merging procedure has started before this transaction
has started then the $\mathit{begin\_transaction}$ operation will
block until the merging is complete.




\paragraph{transactional\_update}
The $\mathit{transactional\_update}$ operation indicates an update operation
performed to the shared data structure during a transaction.
It takes as input an identifier $\mathit{addr}$ of the location to update,
an integer $\mathit{op}$ indicating the operation to perform, and a value $\mathit{val}$.
The operation simply stores the update locally in a hash table, $\mathit{WS}$.
Notice that given our weakened consistency model, these updates are not
immediately propagated to shared memory, instead
they are stored locally until the merging operation takes place.

Updates performed by a process that happen at the same location are ordered by process order and
follow the update semantics of the given operation/data structure.
Additionally, the updates are ordered after the last merging operation that occurred before
the start of the transaction, and any updates performed by other processes after this merge and before
the next are considered to be concurrent with this transaction.


\paragraph{transactional\_read}
The $\mathit{transactional\_read}$ operation is a read to the shared data structure
where each of the reads that occur within a single transaction return the values from
an atomic snapshot of the shared data structure.
It takes as input an identifier $\mathit{addr}$ and returns a value $\mathit{val}$.

Defined for this protocol, the snapshot is the status of shared memory at the time that
the process returns from the from the $\mathit{begin\_transaction}$ operation.
Notice that, since the $\mathit{begin\_transaction}$ operation
sets flags blocking any merging to take place, the shared data structure
will not be modified, thus any values read from this structure obviously come from
an atomic snapshot.
The operation is not quite as simple as directly reading from shared memory though,
as the process might have performed updates that have not yet been merged to shared memory
and process order must still be respected.
Thus, the read starts by checking if the value has been written by this
thread since the last merge by performing a look up in the local write set
(values written before the last merge are cleared from the write set during
the $\mathit{stop\_the\_world}$ operation).
If a value is found in the write set then it is returned as it is more recent
then the value in shared memory, respecting process order.
Otherwise, if no value is found,
the location is read from the shared data structure and that value is returned.



\paragraph{commit}
The $\mathit{commit}$ operation indicates the end of a transaction.
It simple announces that this thread is no longer executing a transaction,
so that a merging operation knows that it does not have to wait for this process
before safely executing.

It is important to notice that it makes no changes to shared memory and that the write set is not cleared,
this way the updates can be kept locally and propagated to shared memory during the next merge operation.
Additionally, future transactions performed by this process will be ordered after this
transactions as such, they must observe the modifications made by this transaction, even if they
have not yet been propagated to shared memory.


\subsection{To consider}
The important thing to consider is that even after a transaction commits, the updates will not
necessarily be visible to other transactions that start afterwards.
This will probably make a big impact on how a programmer will have to write his code, but should
also provide much greater scalability.
Considering programmability, one thing that might be interesting to consider is to allow merging
based on certain events that the programmer might be aware, rather then just performing merging
on some interval.
For example a programmer could even provoke a merge himself if he wanted certain changes to be sure
to be visible when a transaction commits.
Or certain data types might warrant a merge at a certain point in their operation in order
to provide some interesting guarantees (for example possibly in order to ensure 
that a list really is empty??).


\section{STM-like}
The two obvious weaknesses of the \emph{stop the world} algorithm are that
it requires all processes to stop executing transactions while a merging takes place
and that updates done by one process are not immediately visible to other processes.
This section will present a protocol that avoids these shortcomings at the cost
of additional memory management.
The key difference in this protocol is that instead of having a updates be kept
locally until a global merging is perform, the process executing a transactions
perform its updates to shared memory immediately upon commit.
In order to enable transactions to do this while still ensuring that other transactions
observe an atomic state of shared memory, multiple copies of the shared data is kept.
similar to as it would be in a multi-version STM protocols.
STM protocols traditionally keep multiple versions in-order to allow for read-only transactions
to read from an atomic snapshot of memory so that they never abort.
Similarly, in the protocol presented in this section keeping multiple versions allow transactions to read
from an atomic snapshot, but differently, given the weakened consistency requirements, updating transactions
will never abort due to the fact that conflicting concurrent updates are merged.

\subsection{Operations and data structures}
A shared global counter $\mathit{gc}$ is used to order transactions.
Each transaction is assigned a unique time value during commit by performing the atomic
operation $\mathit{incrament\&fetch}$ on $\mathit{gc}$.


The shared copy the data is stored in a hash-table
$\mathit{hash\_table}$ with each object in the hash-table
being stored as a list identified by a unique address $\mathit{addr}$.
Each item in the list describes a version of this object created
by an updating transaction, thus they contain an integer
set to the unique time $\mathit{time}$ given to the transaction that commuted this
update, a value $\mathit{val}$ created by this update, and a $\mathit{next}$ pointer
which points to the node created by the previous update.
Additionally each unique $\mathit{addr}$ has a lock associated with it.


Each process has several local variables.
An integer $\mathit{start\_time}$ is set to the value of $\mathit{GC}$
at the start of the transaction,
this will be the assigned time for the atomic snapshot of the transaction.
Additionally each process contains a write set $\mathit{WS}$
implemented as a hash table
storing all updates that have taken place on this process
since the beginning of the current transaction.

\paragraph{begin\_transaction}
At the beginning of a transaction the local variable $\mathit{start\_time}$
is updated to the value of the shared global counter.
This will be used as the time of the atomic snapshot that is read
by the $\mathit{transactional\_read}$ operations.

\paragraph{transactional\_update}
The $\mathit{transactional\_update}$ operation indicates an update operation
performed to the shared data structure during a transaction.
It takes as input an identifier $\mathit{addr}$ of the location to update,
an integer $\mathit{op}$ indicating the operation to perform, and a value $\mathit{val}$.
The operation simply stores the update locally in a hash table, $\mathit{WS}$.


\paragraph{transactional\_read}
The $\mathit{transactional\_read}$ operation is a read to the shared data structure
where each of the reads that occur within a single transaction return the values from
an atomic snapshot of the shared data structure.
It takes as input an identifier $\mathit{addr}$ and returns a value $\mathit{val}$.

The read starts by checking the write set $\mathit{WS}$ to see if the value has already been written
by this transaction, returning the value written if this is true.
Otherwise, given that there could be concurrent transactions updating the shared structure,
the operation cannot simply just return the most recent value written.
Instead it will return the value that was in memory at the start of this transaction.
In order to do this, first the head of the list for the location $\mathit{addr}$ is retried
from the shared hash-table $\mathit{multi-version-array}$ and the list is then traversed
by the multi-version $\mathit{get}$ operation
until a node with time smaller or equal to $\mathit{start\_time}$ is found and
its value is then returned.

\paragraph{commit}
The commit operation merges the updates done by this transaction
to shared memory.
It starts by acquiring a lock for each of the locations updated by this transaction
(in an order that avoids deadlock).
It then performs a $\mathit{incrament\&fetch}$ operation on the global shared counter,
setting $\mathit{time}$ to the value fetched.
This value is used as the unique time value for the transaction.
Next, each of the updates done by this transaction are merged to shared memory by
calling the $\mathit{merge operation}$.
This operation allocates a new list item for the update, setting its time vale
to the unique time for this transaction.
The updated value for this location stored at this new list item is computed by traversing the list merging
the new operation with each of the updates to this location that took place concurrently i.e. those that happened
after the start of the committing transaction up until the present time.
(Note that it might not be necessary to traverse the list, depending on how the semantics of the
data type and how the information is stored)
The new item is then added to the head of its associated list.
Finally the commit operation finishes by release the locks.


\subsection{To consider}
\paragraph{Multi-version}
Here keeping track of multiple-versions is done by keeping a list.
The $\mathit{merge}$ and $\mathit{get}$ operations shown here are specific to this list structure.
There should be other interesting data structures to use for keeping multiple versions, maybe
even ones that could prevent having to perform access to shared memory that is modified.
It would be nice if, when performing a read, the correct location was accessible without first traversing the newer
list items (i.e. trying to avoid the cache thrashing).

\paragraph{STM protocol}
The design for the multi-version STM protocol is fairly basic here, there are many other designs that might
be interesting to use, for example something closer to NoREC, or
a lock-free design, etc..

\begin{figure}[htb]
\centering{ \fbox{
\begin{minipage}[t]{150mm}
\footnotesize 
\renewcommand{\baselinestretch}{2.5} 
%\resetline
%\setcounter{linecounter}{200}
\begin{tabbing}
aaaaaaa\=aa\=aaaaa\=aa\=aa\=\kill %~\\



{\bf operation} ${\sf begin\_transaction}()$ \\
\line{A01} \> {\bf while} ($\sf{true}$) {\bf do} \\
\line{A01} \>\> $\mathit{stopped} \gets {\sf false}$ \\
\line{A01} \>\> {\bf if} ($\neg \mathit{stop}$) {\bf then} \\
\line{A01} \>\>\> $\mathit{live} \gets {\sf false}$ \\
\line{A01} \>\>\> $\sf return$() \\
\line{A01} \>\> {\bf else} \\
\line{A01} \>\>\> $\mathit{live} \gets \mathit{false}$ \\
\line{A01} \>\>\> $\mathit{stopped} \gets \mathit{true}$ \\
\line{A01} \>\>\> ${\sf wait}(\mathit{stop})$ \\
\line{A01} \> {\bf end if} {\bf end while} \\
{\bf end operation}. \\
\\

{\bf operation} ${\sf transactional\_read}(\mathit{addr})$ \\
\line{A01} \> {\bf if}($\mathit{addr} \in \mathit{WS}$) {\bf then} \\
\line{A01} \>\> ${\sf return}$ ($\mathit{WS.get}(\mathit{addr})$) {\bf end if} \\
\line{A01} \> ${\sf return}$($\mathit{hash\_table.get}(\mathit{addr})$) \\
{\bf end operation}. \\
\\

{\bf operation} ${\sf transactional\_update}(\mathit{addr}, \mathit{op}, \mathit{val})$ \\
\line{A01} \> $\mathit{WS.add}(\mathit{addr}, \mathit{op}, \mathit{val})$ \\
{\bf end operation}. \\
\\

{\bf operation} ${\sf commit}()$ \\
\line{A01} \> $\mathit{live} \gets {\sf false}$ \\
\line{A01} \> $\mathit{stopped} \gets {\sf true}$ \\
\line{A01} \> ${\sf return}$ (${\sf true}$) \\
{\bf end operation}. \\
\\

\end{tabbing}
\normalsize
\end{minipage}
}
\caption{Transactional operations for the stop-the-world version}
\label{fig:ntops}
}
\end{figure}



\begin{figure}[htb]
\centering{ \fbox{
\begin{minipage}[t]{150mm}
\footnotesize 
\renewcommand{\baselinestretch}{2.5} 
%\resetline
%\setcounter{linecounter}{200}
\begin{tabbing}
aaaaaaa\=aa\=aaaaa\=aa\=aa\=\kill %~\\

{\bf operation} ${\sf stop\_the\_world}()$ {\bf is} \\
\line{A01} \> {\bf for each} ($\mathit{p} \in \mathit{processes}$) {\bf do} \\
\line{A01} \>\> $\mathit{p.stop} \gets {\sf true}$ \\
\line{A01} \>\> {\bf while}($\neg \mathit{p.stopped} \cup \mathit{p.live}$); \\
\line{A01} \> {\bf end for} \\
\line{A01} \> {\bf for each} ($\mathit{p} \in \mathit{processes}$) {\bf do} \\
\line{A01} \>\> {\bf for each} ($\mathit{item} \in \mathit{p.WS}$) {\bf do} \\
\line{A01} \>\>\> $\mathit{hash\_table.merge}(\mathit{item.addr}, \mathit{item.op}, \mathit{item.var})$ \\
\line{A01} \>\> {\bf end for} \\
\line{A01} \>\> $\mathit{p.WS.clear}()$ {\bf end for} \\
\line{A01} \> {\bf for each} ($\mathit{p} \in \mathit{processes}$) {\bf do} \\
\line{A01} \>\> $\mathit{p.stopped} \gets {\sf false}$; $\mathit{p.stop} \gets {\sf false}$ \\
{\bf end operation}. \\
\\


\end{tabbing}
\normalsize
\end{minipage}
}
\caption{Operation to stop the world and merge the updates}
\label{fig:ntops}
}
\end{figure}


\begin{figure}[htb]
\centering{ \fbox{
\begin{minipage}[t]{150mm}
\footnotesize 
\renewcommand{\baselinestretch}{2.5} 
%\resetline
%\setcounter{linecounter}{200}
\begin{tabbing}
aaaaaaa\=aa\=aaaaa\=aa\=aa\=\kill %~\\


{\bf operation} ${\sf begin\_transaction}()$ \\
\line{A01} \> {\bf if} ($\mathit{WS.is\_empty}()$) {\bf then} \\
\line{A01} \>\> $\mathit{start\_time} \gets \mathit{GC}$ {\bf end if} \\
{\bf end operation}. \\
\\

{\bf operation} ${\sf transactional\_read}(\mathit{addr})$ \\
\line{A01} \> {\bf if}($\mathit{addr} \in \mathit{WS}$) {\bf then} \\
\line{A01} \>\> ${\sf return}$ ($\mathit{WS.get}(\mathit{addr})$) {\bf end if} \\
\line{A01} \> ${\sf return}$($\mathit{multi\_version\_struct.get}(\mathit{addr})$) \\
{\bf end operation}. \\
\\

{\bf operation} ${\sf transactional\_update}(\mathit{addr}, \mathit{op}, \mathit{val})$ \\
\line{A01} \> $\mathit{WS.add}(\mathit{addr}, \mathit{op}, \mathit{val})$ \\
{\bf end operation}. \\
\\

{\bf operation} ${\sf commit}()$ \\

\line{A01} \> {\bf for each} ($\mathit{item} \in \mathit{WS}$) {\bf do} \\
\line{A01} \>\> ${\sf lock}(\mathit{item.addr})$ {\bf end for} \\
\line{A01} \> $\mathit{time} \gets {\sf incrament\&fetch}(\mathit{GC})$ \\
\line{A01} \> {\bf for each} ($\mathit{item} \in \mathit{WS}$) {\bf do} \\
\line{A01} \>\> $\mathit{multi\_version\_array.merge}(\mathit{item.addr}, \mathit{item.op}, \mathit{item.val}, \mathit{start\_time}, \mathit{time})$ {\bf end for} \\
\line{A01} \> {\bf for each} ($\mathit{item} \in \mathit{WS}$) {\bf do} \\
\line{A01} \>\> ${\sf unlock}(\mathit{item.addr})$ {\bf end for} \\
\line{A01} \> $\mathit{WS.clear}()$ \\
\line{A01} \> ${\sf return}$ (${\sf false}$) \\
{\bf end operation}. \\
\\



\end{tabbing}
\normalsize
\end{minipage}
}
\caption{Transactional operations for the more traditional STM version}
\label{fig:ntops}
}
\end{figure}




\begin{figure}[htb]
\centering{ \fbox{
\begin{minipage}[t]{150mm}
\footnotesize 
\renewcommand{\baselinestretch}{2.5} 
%\resetline
%\setcounter{linecounter}{200}
\begin{tabbing}
aaaaaaa\=aa\=aaaaa\=aa\=aa\=\kill %~\\




{\bf operation}  ${\sf get}(\mathit{addr}, \mathit{time})$ {\bf is}\\
\line{A01} \> $\mathit{next} \gets \mathit{hash\_table.get}(\mathit{addr})$ \\
\line{A01} \> {\bf while}($\mathit{time} < \mathit{next.time}$){\bf do} \\
\line{A01} \>\> $\mathit{next} \gets \mathit{next.time}$ {\bf end while} \\
\line{A01} \> ${\sf return}(\mathit{next.val})$ \\
{\bf end operation}. \\
\\


{\bf operation} ${\sf merge}(\mathit{addr}, \mathit{op}, \mathit{val}, \mathit{start\_time}, \mathit{time})$ \\
\line{A01} \> $\mathit{next} \gets \mathit{hash\_table.get}(\mathit{addr})$ \\
\line{A01} \> $\mathit{node} \gets$ allocate new node \\
\line{A01} \> $\mathit{node.time} \gets \mathit{time}$; $\mathit{node.val} \gets \mathit{val}$ \\
\line{A01} \> $\mathit{node.next} \gets \mathit{next}$ \\
\line{A01} \> {\bf while}($\mathit{start\_time} < \mathit{next.time}$){\bf do} \\
\line{A01} \>\> $\mathit{merge}(\mathit{op}, \mathit{node}, \mathit{next})$ \\
\line{A01} \>\> $\mathit{next} \gets \mathit{next.next}$ {\bf end while} \\
\line{A01} \> $\mathit{merge}(\mathit{op}, \mathit{node}, \mathit{next})$ \\
\line{A01} \> $\mathit{hash\_table.set}(\mathit{addr}, \mathit{node})$ \\
{\bf end operation}. \\
\\





\end{tabbing}
\normalsize
\end{minipage}
}
\caption{Operations of the multi-version structure}
\label{fig:ntops}
}
\end{figure}


ToDo:  Find the interesting intersections of these algorithms to see what parts are good/bad
for scalibiltiy and different applications.
Benchmarking should also help with this.

Idea:  See how these relate to graph processing algorithms from parallel processing
conference papers (eg from PPoPP) to see if there is an application here.


Idea: in all protocols need to do a memory barrier at transaction start/end (to let know safely that the transaction is no longer executing)
Is there a way we can take advantage of this in the memory structure?
Would want to be able to read directly from a time in memory without querying the location from the central structure that could have been updated
in the mean-time.


ToDo: Would it be interesting to add strong consistency along side weak, how would this model work?  Could we use it to implement
certain data structures?
I.e. a check if empty operation, or a producer/single consumer.
Can you do some strong read/writes within a transaction also containing weak read/writes?


\end{document}